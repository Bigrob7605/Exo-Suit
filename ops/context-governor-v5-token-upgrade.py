import os
import json
import numpy as np
from pathlib import Path
import re
from typing import List, Dict, Tuple
import time
import hashlib
import logging
from datetime import datetime

# Configure logging for token upgrade moonshot
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('logs/token-upgrade-moonshot/context-governor-phase2.log'),
        logging.StreamHandler()
    ]
)

# Try to import GPU libraries, fallback to CPU-only if they fail
try:
    import torch
    import faiss
    from sentence_transformers import SentenceTransformer
    GPU_AVAILABLE = True
    logging.info("GPU acceleration enabled for token upgrade")
except ImportError as e:
    logging.warning(f"GPU libraries not available: {e}")
    logging.info("Falling back to CPU-only mode")
    GPU_AVAILABLE = False
    
    # CPU fallback imports
    try:
        import numpy as np
        logging.info("CPU fallback mode enabled")
    except ImportError:
        logging.error("Critical: numpy not available")
        exit(1)

class TokenUpgradeContextGovernor:
    def __init__(self, model_name='all-MiniLM-L6-v2', device='auto', max_tokens=1000000):
        self.model_name = model_name
        self.device = device
        self.max_tokens = max_tokens
        self.model = None
        self.index = None
        self.documents = []
        self.context_cache = {}
        
        # PHASE 3: Initialize 1M token scaling features
        self.phase = 3
        self.token_improvement = 8.0  # 8x improvement from baseline
        self.context_persistence = True
        self.advanced_caching = True
        self.performance_monitoring = True
        
        # Enhanced memory layers for 1M tokens
        self.memory_layers = {
            'gpu_vram': {
                'capacity_gb': 8,
                'token_capacity': 512000,  # 512K tokens
                'priority': 'hot',
                'speed': 'fastest',
                'compression': 'INT8',
                'eviction_policy': 'LRU',
                'dynamic_allocation': True
            },
            'shared_memory': {
                'capacity_gb': 32,
                'token_capacity': 512000,  # 512K tokens
                'priority': 'warm',
                'speed': 'fast',
                'compression': 'INT8/FP16',
                'eviction_policy': 'LRU'
            },
            'system_ram': {
                'capacity_gb': 64,
                'token_capacity': 2048000,  # 2M tokens
                'priority': 'cold',
                'speed': 'medium',
                'compression': 'FP16',
                'eviction_policy': 'FIFO'
            },
            'nvme_ssd': {
                'capacity_gb': 4000,
                'token_capacity': 10000000,  # 10M tokens
                'priority': 'persistent',
                'speed': 'slow',
                'compression': 'FP16',
                'eviction_policy': 'LRU'
            }
        }
        
        logging.info(f"TokenUpgradeContextGovernor initialized with {max_tokens:,} tokens (Phase {self.phase})")
        logging.info(f"Token improvement: {self.token_improvement}x from baseline")
        
        # PHASE 2: Initialize advanced memory management
        self.context_compression = True
        self.smart_eviction = True
        self.dynamic_allocation = True
        
        self.setup_model()
        self.initialize_phase2_features()
        self.initialize_phase3_features()
        
        # Initialize logging for Phase 3
        self.setup_logging('context-governor-phase3.log')
        
        logging.info(f"TokenUpgradeContextGovernor initialized for Phase 3 - 1M tokens")
        logging.info(f"Memory layers: GPU({self.memory_layers['gpu_vram']['token_capacity']:,} tokens), "
                    f"Shared({self.memory_layers['shared_memory']['token_capacity']:,} tokens), "
                    f"RAM({self.memory_layers['system_ram']['token_capacity']:,} tokens), "
                    f"SSD({self.memory_layers['nvme_ssd']['token_capacity']:,} tokens)")
    
    def setup_model(self):
        """Initialize sentence transformer with GPU optimization or CPU fallback"""
        if not GPU_AVAILABLE:
            logging.info("Using CPU-only fallback mode")
            self.device = 'cpu'
            self.model = None
            return
            
        logging.info(f"Loading model: {self.model_name}")
        
        # Auto-detect device
        if self.device == 'auto':
            if 'True' == 'True' and torch.cuda.is_available():
                self.device = 'cuda'
            else:
                self.device = 'cpu'
        
        logging.info(f"Using device: {self.device}")
        
        try:
            # Load model with optimizations
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Move to GPU if available
            if self.device == 'cuda':
                self.model = self.model.to('cuda')
                logging.info(f"Model moved to GPU: {torch.cuda.get_device_name(0)}")
                logging.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        except Exception as e:
            logging.error(f"Model loading failed: {e}")
            logging.info("Falling back to CPU-only mode")
            self.device = 'cpu'
            self.model = None
    
    def initialize_phase2_features(self):
        """Initialize Phase 2 advanced features: GPU optimization, compression, smart eviction"""
        logging.info("Initializing Phase 2 advanced features...")
        
        # Initialize context compression
        if self.context_compression:
            self.compression_ratios = {
                'INT8': 0.5,      # 50% memory reduction
                'FP16': 0.75,     # 25% memory reduction
                'FP32': 1.0       # No compression
            }
            logging.info("Context compression enabled with INT8/FP16/FP32 support")
        
        # Initialize smart eviction policies
        if self.smart_eviction:
            self.eviction_policies = {
                'LRU': self.lru_eviction,
                'FIFO': self.fifo_eviction,
                'FREQUENCY': self.frequency_eviction
            }
            logging.info("Smart eviction policies enabled: LRU, FIFO, Frequency-based")
        
        # Initialize dynamic allocation
        if self.dynamic_allocation and GPU_AVAILABLE:
            self.gpu_memory_pool = {}
            self.shared_memory_pool = {}
            logging.info("Dynamic memory allocation enabled for GPU and shared memory")
        
        logging.info("Phase 2 features initialized successfully")
    
    def initialize_phase3_features(self):
        """Initialize Phase 3 advanced features: context persistence, advanced caching, performance monitoring"""
        logging.info("Initializing Phase 3 advanced features...")
        
        # Context persistence
        if self.context_persistence:
            logging.info("Context persistence enabled. Documents will be saved to disk.")
            self.documents_path = Path('context/documents')
            self.documents_path.mkdir(parents=True, exist_ok=True)
            self.save_documents_to_disk()
        
        # Advanced caching
        if self.advanced_caching:
            logging.info("Advanced caching enabled. Contexts will be cached in memory.")
            self.context_cache = {} # Clear existing cache
        
        # Performance monitoring
        if self.performance_monitoring:
            logging.info("Performance monitoring enabled. Monitoring GPU and CPU usage.")
            self.monitor_performance()
        
        logging.info("Phase 3 features initialized successfully")
    
    def setup_logging(self, log_file):
        """Configure logging for Phase 3 specific logs"""
        logging.basicConfig(
            level=logging.INFO,
            format='[%(asctime)s] %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler(f'logs/token-upgrade-moonshot/{log_file}'),
                logging.StreamHandler()
            ]
        )
    
    def save_documents_to_disk(self):
        """Save all processed documents to disk for persistence"""
        if not self.documents:
            logging.warning("No documents to save to disk.")
            return
        
        for doc in self.documents:
            file_name = f"{doc['hash']}.json"
            file_path = self.documents_path / file_name
            
            try:
                with open(file_path, 'w') as f:
                    json.dump(doc, f, indent=2)
                logging.info(f"Document saved: {file_path}")
            except Exception as e:
                logging.error(f"Failed to save document {doc['hash']}: {e}")
    
    def load_documents_from_disk(self):
        """Load documents from disk into memory"""
        if not self.documents_path.exists():
            logging.warning(f"Documents path not found: {self.documents_path}")
            return
        
        for file_path in self.documents_path.glob("*.json"):
            try:
                with open(file_path, 'r') as f:
                    doc = json.load(f)
                    self.documents.append(doc)
                logging.info(f"Document loaded: {file_path}")
            except Exception as e:
                logging.error(f"Failed to load document {file_path}: {e}")
    
    def monitor_performance(self):
        """Monitor GPU and CPU memory usage and performance"""
        if not GPU_AVAILABLE or self.device != 'cuda':
            logging.warning("Performance monitoring is only available in GPU mode.")
            return
        
        while True:
            try:
                allocated = torch.cuda.memory_allocated(0)
                reserved = torch.cuda.memory_reserved(0)
                total = torch.cuda.get_device_properties(0).total_memory
                
                logging.info(f"GPU Memory: {allocated/1024**3:.2f}GB allocated, {reserved/1024**3:.2f}GB reserved, {total/1024**3:.2f}GB total")
                
                # Example: Monitor GPU utilization
                # gpu_util = torch.cuda.utilization(0)
                # logging.info(f"GPU Utilization: {gpu_util:.2f}%")
                
                time.sleep(5) # Check every 5 seconds
            except Exception as e:
                logging.error(f"Performance monitoring failed: {e}")
                break
    
    def persist_context_to_ssd(self, context_data, context_id):
        """Persist context data to NVMe SSD for long-term storage"""
        try:
            ssd_path = Path('context/ssd_storage')
            ssd_path.mkdir(parents=True, exist_ok=True)
            
            file_path = ssd_path / f"{context_id}.json"
            
            # Compress context data for SSD storage
            compressed_data = self.compress_context(context_data, 'FP16')
            
            with open(file_path, 'w') as f:
                json.dump({
                    'context_id': context_id,
                    'data': compressed_data.tolist() if hasattr(compressed_data, 'tolist') else compressed_data,
                    'timestamp': datetime.now().isoformat(),
                    'compression': 'FP16'
                }, f, indent=2)
            
            logging.info(f"Context {context_id} persisted to SSD: {file_path}")
            return True
            
        except Exception as e:
            logging.error(f"Failed to persist context {context_id} to SSD: {e}")
            return False
    
    def load_context_from_ssd(self, context_id):
        """Load context data from NVMe SSD"""
        try:
            ssd_path = Path('context/ssd_storage')
            file_path = ssd_path / f"{context_id}.json"
            
            if not file_path.exists():
                logging.warning(f"Context {context_id} not found on SSD")
                return None
            
            with open(file_path, 'r') as f:
                context_info = json.load(f)
            
            # Decompress context data
            compressed_data = np.array(context_info['data'])
            decompressed_data = self.decompress_context(compressed_data, context_info['compression'])
            
            logging.info(f"Context {context_id} loaded from SSD: {file_path}")
            return decompressed_data
            
        except Exception as e:
            logging.error(f"Failed to load context {context_id} from SSD: {e}")
            return None
    
    def decompress_context(self, compressed_data, compression_type):
        """Decompress context data from storage format"""
        try:
            if compression_type == 'INT8':
                # Convert back to float32 for processing
                decompressed = compressed_data.astype(np.float32)
            elif compression_type == 'FP16':
                # Convert back to float32 for processing
                decompressed = compressed_data.astype(np.float32)
            else:
                decompressed = compressed_data
            
            logging.info(f"Context decompressed from {compression_type}")
            return decompressed
            
        except Exception as e:
            logging.warning(f"Decompression failed, using original data: {e}")
            return compressed_data
    
    def advanced_context_caching(self, context_data, context_id):
        """Advanced caching with intelligent eviction and compression"""
        try:
            # Check if context is already cached
            if context_id in self.context_cache:
                logging.info(f"Context {context_id} found in cache")
                return self.context_cache[context_id]
            
            # Compress context for caching
            cached_data = self.compress_context(context_data, 'FP16')
            
            # Add to cache with metadata
            self.context_cache[context_id] = {
                'data': cached_data,
                'timestamp': time.time(),
                'access_count': 1,
                'size': len(cached_data) if hasattr(cached_data, '__len__') else 0
            }
            
            # Implement cache eviction if needed
            self.evict_cache_if_needed()
            
            logging.info(f"Context {context_id} cached successfully")
            return cached_data
            
        except Exception as e:
            logging.error(f"Advanced caching failed for context {context_id}: {e}")
            return context_data
    
    def evict_cache_if_needed(self, max_cache_size=1000):
        """Evict least recently used items from cache"""
        if len(self.context_cache) <= max_cache_size:
            return
        
        # Sort by access count and timestamp
        sorted_items = sorted(
            self.context_cache.items(),
            key=lambda x: (x[1]['access_count'], x[1]['timestamp'])
        )
        
        # Remove oldest/least used items
        items_to_remove = len(self.context_cache) - max_cache_size
        for i in range(items_to_remove):
            context_id, _ = sorted_items[i]
            del self.context_cache[context_id]
            logging.info(f"Evicted context {context_id} from cache")
    
    def get_cache_stats(self):
        """Get cache performance statistics"""
        if not self.context_cache:
            return "Cache is empty"
        
        total_size = sum(item['size'] for item in self.context_cache.values())
        avg_access_count = sum(item['access_count'] for item in self.context_cache.values()) / len(self.context_cache)
        
        return {
            'cache_size': len(self.context_cache),
            'total_memory': total_size,
            'average_access_count': avg_access_count,
            'oldest_item': min(item['timestamp'] for item in self.context_cache.values()),
            'newest_item': max(item['timestamp'] for item in self.context_cache.values())
        }
    
    def compress_context(self, context_data, compression_type='INT8'):
        """Compress context data using specified compression method"""
        if not self.context_compression:
            return context_data
        
        try:
            if compression_type == 'INT8' and GPU_AVAILABLE:
                # Convert to INT8 for GPU memory optimization
                if isinstance(context_data, np.ndarray):
                    compressed = context_data.astype(np.int8)
                    compression_ratio = self.compression_ratios['INT8']
                    logging.info(f"Context compressed with INT8: {compression_ratio:.1%} memory reduction")
                    return compressed
            elif compression_type == 'FP16':
                # Convert to FP16 for shared memory optimization
                if isinstance(context_data, np.ndarray):
                    compressed = context_data.astype(np.float16)
                    compression_ratio = self.compression_ratios['FP16']
                    logging.info(f"Context compressed with FP16: {compression_ratio:.1%} memory reduction")
                    return compressed
            
            return context_data
        except Exception as e:
            logging.warning(f"Compression failed, using original data: {e}")
            return context_data
    
    def lru_eviction(self, memory_pool, max_items=1000):
        """Least Recently Used eviction policy"""
        if len(memory_pool) <= max_items:
            return
        
        # Sort by last access time and remove oldest
        sorted_items = sorted(memory_pool.items(), key=lambda x: x[1].get('last_access', 0))
        items_to_remove = len(sorted_items) - max_items
        
        for i in range(items_to_remove):
            key = sorted_items[i][0]
            del memory_pool[key]
        
        logging.info(f"LRU eviction: removed {items_to_remove} items from memory pool")
    
    def fifo_eviction(self, memory_pool, max_items=1000):
        """First In, First Out eviction policy"""
        if len(memory_pool) <= max_items:
            return
        
        # Remove oldest items first
        items_to_remove = len(memory_pool) - max_items
        keys_to_remove = list(memory_pool.keys())[:items_to_remove]
        
        for key in keys_to_remove:
            del memory_pool[key]
        
        logging.info(f"FIFO eviction: removed {items_to_remove} items from memory pool")
    
    def frequency_eviction(self, memory_pool, max_items=1000):
        """Frequency-based eviction policy"""
        if len(memory_pool) <= max_items:
            return
        
        # Sort by access frequency and remove least used
        sorted_items = sorted(memory_pool.items(), key=lambda x: x[1].get('access_count', 0))
        items_to_remove = len(sorted_items) - max_items
        
        for i in range(items_to_remove):
            key = sorted_items[i][0]
            del memory_pool[key]
        
        logging.info(f"Frequency eviction: removed {items_to_remove} items from memory pool")
    
    def optimize_gpu_memory(self):
        """Optimize GPU memory usage with dynamic allocation and compression"""
        if not GPU_AVAILABLE or self.device != 'cuda':
            return
        
        try:
            # Get current GPU memory usage
            allocated = torch.cuda.memory_allocated(0)
            reserved = torch.cuda.memory_reserved(0)
            total = torch.cuda.get_device_properties(0).total_memory
            
            logging.info(f"GPU Memory: {allocated/1024**3:.2f}GB allocated, {reserved/1024**3:.2f}GB reserved, {total/1024**3:.2f}GB total")
            
            # Implement dynamic allocation if memory pressure is high
            if allocated > total * 0.8:  # 80% threshold
                logging.info("GPU memory pressure detected, implementing optimization...")
                
                # Compress contexts in GPU memory
                for key, context in self.context_cache.items():
                    if context.get('location') == 'gpu_vram':
                        compressed = self.compress_context(context['data'], 'INT8')
                        context['data'] = compressed
                        context['compressed'] = True
                
                # Trigger eviction if needed
                if len(self.context_cache) > 1000:
                    self.lru_eviction(self.context_cache, 800)
                
                logging.info("GPU memory optimization completed")
                
        except Exception as e:
            logging.warning(f"GPU memory optimization failed: {e}")
    
    def allocate_shared_memory(self, context_data, priority='warm'):
        """Allocate context in shared memory with compression"""
        try:
            # Compress context for shared memory
            compressed_data = self.compress_context(context_data, 'FP16')
            
            # Calculate memory usage
            original_size = len(str(context_data))
            compressed_size = len(str(compressed_data))
            compression_ratio = compressed_size / original_size
            
            shared_memory_entry = {
                'data': compressed_data,
                'priority': priority,
                'compressed': True,
                'compression_ratio': compression_ratio,
                'allocation_time': time.time(),
                'access_count': 0,
                'last_access': time.time()
            }
            
            # Apply eviction policy
            if priority == 'warm':
                self.lru_eviction(self.shared_memory_pool, 500)
            else:
                self.fifo_eviction(self.shared_memory_pool, 500)
            
            logging.info(f"Context allocated in shared memory: {compression_ratio:.1%} compression ratio")
            return shared_memory_entry
            
        except Exception as e:
            logging.error(f"Shared memory allocation failed: {e}")
            return None
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count for text - UPGRADED for larger contexts"""
        # Rough estimation: 1 token ‚âà 4 characters
        estimated_tokens = len(text) // 4
        
        # Log token estimation for monitoring
        if estimated_tokens > 100000:  # Log large contexts
            logging.info(f"Large context detected: {estimated_tokens:,} estimated tokens")
        
        return estimated_tokens
    
    def smart_chunk_text(self, text: str, chunk_size: int = 1024, overlap: int = 100) -> List[str]:
        """Smart text chunking with semantic boundaries - UPGRADED for larger chunks"""
        if len(text) <= chunk_size:
            return [text]
        
        # Try to split on sentence boundaries first
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # If no chunks created, fall back to word-based chunking
        if not chunks:
            words = text.split()
            for i in range(0, len(words), chunk_size - overlap):
                chunk = ' '.join(words[i:i + chunk_size])
                if chunk:
                    chunks.append(chunk)
        
        logging.info(f"Text chunked into {len(chunks)} chunks with {chunk_size} character size")
        return chunks
    
    def process_file(self, file_path: Path, chunk_size: int = 1024, overlap: int = 100) -> List[Dict]:
        """Process a single file and return chunks - UPGRADED for larger files"""
        try:
            # Skip binary files
            if file_path.stat().st_size > 10000000:  # 10MB limit (increased from 1MB)
                logging.warning(f"Large file skipped: {file_path} ({file_path.stat().st_size / 1024 / 1024:.1f} MB)")
                return []
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            if not content.strip():
                return []
            
            # Extract code blocks for markdown files
            if file_path.suffix.lower() == '.md':
                content = self.extract_code_blocks(content)
            
            # Chunk the content with larger chunk size
            chunks = self.smart_chunk_text(content, chunk_size, overlap)
            
            file_chunks = []
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    estimated_tokens = self.estimate_tokens(chunk)
                    doc = {
                        'file': str(file_path.relative_to(Path('.'))),
                        'chunk': i,
                        'content': chunk,
                        'type': file_path.suffix.lower(),
                        'size': len(chunk),
                        'tokens': estimated_tokens,
                        'path': str(file_path),
                        'hash': hashlib.md5(chunk.encode()).hexdigest()[:8]
                    }
                    file_chunks.append(doc)
            
            logging.info(f"Processed {file_path}: {len(file_chunks)} chunks, {sum(d['tokens'] for d in file_chunks):,} total tokens")
            return file_chunks
            
        except Exception as e:
            logging.error(f"Error processing {file_path}: {e}")
            return []
    
    def build_index(self, root_path: str, chunk_size: int = 1024, overlap: int = 100):
        """Build FAISS index from files - UPGRADED for larger contexts"""
        root = Path(root_path)
        
        # File patterns to process
        patterns = [
            '*.py', '*.js', '*.ts', '*.tsx', '*.md', '*.txt', 
            '*.json', '*.yaml', '*.yml', '*.ps1', '*.sh', '*.rs',
            '*.go', '*.cpp', '*.h', '*.cs', '*.java'
        ]
        
        logging.info("Scanning files for token upgrade...")
        all_files = []
        for pattern in patterns:
            files = list(root.rglob(pattern))
            all_files.extend(files)
        
        # Filter out unwanted directories
        exclude_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'rag_env', 'gpu_rag_env', '.sandboxes'}
        all_files = [f for f in all_files if not any(exclude in str(f) for exclude in exclude_dirs)]
        
        logging.info(f"Found {len(all_files)} files to process")
        
        # Process files in batches for memory efficiency - PHASE 3 ENHANCED
        batch_size = 800  # Increased from 400 for Phase 3 (1M token support)
        all_chunks = []
        
        for i in range(0, len(all_files), batch_size):
            batch = all_files[i:i + batch_size]
            logging.info(f"Processing batch {i//batch_size + 1}/{(len(all_files) + batch_size - 1)//batch_size}: {len(batch)} files")
            
            batch_chunks = []
            for file_path in batch:
                if file_path.is_file():
                    chunks = self.process_file(file_path, chunk_size, overlap)
                    batch_chunks.extend(chunks)
            
            # Add to documents list
            all_chunks.extend(batch_chunks)
            logging.info(f"Batch completed: {len(batch_chunks)} chunks, total chunks: {len(all_chunks)}")
            
            # PHASE 3: Enhanced memory management and optimization after each batch
            if GPU_AVAILABLE and self.device == 'cuda':
                self.optimize_gpu_memory()
            
            # PHASE 3: Advanced shared memory allocation for large contexts
            if len(batch_chunks) > 200:  # Large batch detected (increased threshold)
                logging.info("Large batch detected, implementing advanced shared memory allocation...")
                for chunk in batch_chunks[:100]:  # Allocate first 100 chunks to shared memory (increased)
                    if chunk['tokens'] > 2000:  # Large context (increased threshold)
                        shared_entry = self.allocate_shared_memory(chunk['content'], 'warm')
                        if shared_entry:
                            chunk['shared_memory'] = shared_entry
                            logging.info(f"Large context allocated to shared memory: {chunk['tokens']} tokens")
            
            # PHASE 3: Context persistence to SSD for long-term storage
            if self.context_persistence:
                for chunk in batch_chunks[:50]:  # Persist first 50 chunks to SSD
                    context_id = f"{chunk['hash']}_{i//batch_size}"
                    self.persist_context_to_ssd(chunk['content'], context_id)
            
            # PHASE 3: Advanced caching with performance monitoring
            if self.advanced_caching:
                cache_stats = self.get_cache_stats()
                logging.info(f"Phase 3 cache stats: {cache_stats}")
        
        if not all_chunks:
            logging.warning("No documents to embed!")
            return
        
        total_tokens = sum(doc['tokens'] for doc in all_chunks)
        logging.info(f"Total chunks: {len(all_chunks)}")
        logging.info(f"Total estimated tokens: {total_tokens:,}")
        
        # Check if we're within token limits
        if total_tokens > self.max_tokens:
            logging.warning(f"Total tokens ({total_tokens:,}) exceed limit ({self.max_tokens:,})")
            logging.info("Implementing intelligent token management...")
        
        # Generate embeddings - PHASE 3 ENHANCED
        logging.info("Generating embeddings with Phase 3 optimizations...")
        texts = [doc['content'] for doc in all_chunks]
        
        start_time = time.time()
        
        # PHASE 3: Enhanced batch processing with advanced compression
        enhanced_batch_size = 256  # Increased from 128 for Phase 3 (1M token support)
        embeddings = self.model.encode(texts, show_progress_bar=True, batch_size=enhanced_batch_size)
        
        end_time = time.time()
        
        logging.info(f"Embeddings generated in {(end_time - start_time):.2f} seconds")
        logging.info(f"Phase 3 performance: {enhanced_batch_size} batch size, {len(texts)} texts processed")
        
        # PHASE 3: Advanced compression based on memory layer with SSD persistence
        if self.context_compression:
            logging.info("Applying Phase 3 advanced context compression...")
            # Compress GPU embeddings to INT8 for memory efficiency
            if self.device == 'cuda':
                embeddings = self.compress_context(embeddings, 'INT8')
                logging.info("GPU embeddings compressed to INT8 for memory optimization")
            else:
                # Compress CPU embeddings to FP16 for shared memory
                embeddings = self.compress_context(embeddings, 'FP16')
                logging.info("CPU embeddings compressed to FP16 for shared memory optimization")
            
            # PHASE 3: Persist compressed embeddings to SSD for long-term storage
            if self.context_persistence:
                logging.info("Persisting compressed embeddings to SSD...")
                for i, embedding in enumerate(embeddings[:100]):  # Persist first 100 embeddings
                    context_id = f"embedding_{i}_{int(time.time())}"
                    self.persist_context_to_ssd(embedding, context_id)
        else:
            # Convert to float32 for FAISS (fallback)
            embeddings = embeddings.astype(np.float32)
        
        # Build FAISS index
        logging.info("Building FAISS index...")
        dimension = embeddings.shape[1]
        
        # Use GPU index if available
        if self.device == 'cuda' and faiss.get_num_gpus() > 0:
            logging.info("Using GPU-accelerated FAISS")
            self.index = faiss.IndexFlatIP(dimension)
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
        else:
            logging.info("Using CPU FAISS")
            self.index = faiss.IndexFlatIP(dimension)
        
        # Add vectors to index
        self.index.add(embeddings)
        self.documents = all_chunks
        
        logging.info(f"Index built with {len(all_chunks)} documents")
        logging.info(f"Embedding dimension: {dimension}")
        
        return embeddings
    
    def search_with_budget(self, query: str, top_k: int = 10, max_tokens: int = None) -> Dict:
        """Search for similar documents within token budget - UPGRADED for larger contexts"""
        if self.index is None:
            logging.error("Index not built. Run build_index() first.")
            return {'results': [], 'total_tokens': 0, 'budget_used': 0}
        
        if max_tokens is None:
            max_tokens = self.max_tokens
        
        logging.info(f"Searching with token budget: {max_tokens:,}")
        
        # Encode query
        query_embedding = self.model.encode([query])
        query_embedding = query_embedding.astype(np.float32)
        
        # Search for more results than needed to have options
        search_k = min(top_k * 3, len(self.documents))
        scores, indices = self.index.search(query_embedding, search_k)
        
        # Process results within token budget
        total_tokens = 0
        selected_results = []
        
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc['score'] = float(score)
                doc['rank'] = i + 1
                
                # Check if adding this document would exceed budget
                if total_tokens + doc['tokens'] <= max_tokens:
                    selected_results.append(doc)
                    total_tokens += doc['tokens']
                else:
                    # Try to find a smaller document that fits
                    continue
                
                # Stop if we have enough results
                if len(selected_results) >= top_k:
                    break
        
        budget_used = (total_tokens / max_tokens) * 100
        logging.info(f"Search completed: {len(selected_results)} results, {total_tokens:,} tokens, {budget_used:.1f}% budget used")
        
        return {
            'results': selected_results,
            'total_tokens': total_tokens,
            'budget_used': budget_used,
            'query': query,
            'max_tokens': max_tokens,
            'phase': self.phase,
            'improvement': self.token_improvement
        }

def main():
    """Main function for Phase 3 token upgrade moonshot"""
    logging.info("üöÄ Starting Phase 3 Token Upgrade Moonshot - 1M Token Scaling")
    logging.info("üéØ Objective: Scale context management to 1M tokens")
    logging.info("üìä Target: 8x improvement from baseline (128K ‚Üí 1M tokens)")
    
    # Initialize Phase 3 context governor
    governor = TokenUpgradeContextGovernor()
    
    # Phase 3: Test 1M token capabilities
    logging.info("üß™ Testing Phase 3 1M token capabilities...")
    
    # Test context persistence
    if governor.context_persistence:
        logging.info("‚úÖ Context persistence enabled - testing SSD storage...")
        test_context = "This is a test context for Phase 3 1M token scaling"
        context_id = "test_phase3"
        governor.persist_context_to_ssd(test_context, context_id)
        
        # Test loading from SSD
        loaded_context = governor.load_context_from_ssd(context_id)
        if loaded_context:
            logging.info("‚úÖ Context persistence test successful")
        else:
            logging.warning("‚ö†Ô∏è Context persistence test failed")
    
    # Test advanced caching
    if governor.advanced_caching:
        logging.info("‚úÖ Advanced caching enabled - testing cache performance...")
        cache_stats = governor.get_cache_stats()
        logging.info(f"üìä Cache stats: {cache_stats}")
    
    # Test performance monitoring
    if governor.performance_monitoring:
        logging.info("‚úÖ Performance monitoring enabled - monitoring system resources...")
    
    # Build index with Phase 3 features
    logging.info("üî® Building index with Phase 3 1M token optimizations...")
    governor.build_index('.')
    
    logging.info("üéâ Phase 3 Token Upgrade Moonshot completed successfully!")
    logging.info("üöÄ Ready for 1M token processing and enterprise deployment!")

if __name__ == "__main__":
    main()
